# -*- coding: utf-8 -*-
"""Group 1_Task2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fmCjdODtqyGDDV4ud7gwCsaPXoY35Jxo

# 1. Environment setup and data preparation
"""

#!pip install transformers datasets scikit-learn pandas tqdm

from google.colab import drive
drive.mount('/content/drive/')

!unzip "/content/drive/MyDrive/SPH6004-group assignment/mimic-cxr-reports.zip"

reports_dir = "/content/files"
csv_file = "/content/drive/MyDrive/SPH6004-group assignment/mimic-cxr-2.0.0-chexpert.csv"

"""# 2. Test data loading and directory structure viewing"""

import os

for subfolder1 in os.listdir(reports_dir):
    subfolder1_path = os.path.join(reports_dir, subfolder1)
    if os.path.isdir(subfolder1_path):
        print(f"üîπ {subfolder1}/")
        for subfolder2 in os.listdir(subfolder1_path):
            subfolder2_path = os.path.join(subfolder1_path, subfolder2)
            print(f"    ‚îî‚îÄ‚îÄ {subfolder2}/")
            for file in os.listdir(subfolder2_path):
                if file.endswith(".txt"):
                    print(f"        üìÑ {file}")
                    break
            break

"""# 3. Data preprocessing and label conversion"""

print(f"Reports directory: {reports_dir}")
import os
import pandas as pd
for subfolder1 in os.listdir(reports_dir):
    print(subfolder1)

import os
import pandas as pd

df = pd.read_csv(csv_file)
df = df.fillna(0)
df.replace(-1, 0, inplace=True)
labels = df.columns[2:]

def read_report_text(row):
    subject_id = str(row['subject_id']).zfill(8)
    study_id = str(row['study_id'])
    report_path = os.path.join(
        reports_dir,
        f"p{subject_id[:2]}",
        f"p{subject_id}",
        f"s{study_id}.txt"
    )
    try:
        with open(report_path, "r") as file:
            return file.read()
    except FileNotFoundError:
        return

df["report_path"] = df.apply(lambda row: os.path.join(
    reports_dir,
    f"p{str(row['subject_id']).zfill(8)[:2]}",
    f"p{str(row['subject_id']).zfill(8)}",
    f"s{int(row['study_id'])}.txt"
), axis=1)

print(df["report_path"].head())

df['report_text'] = df.apply(read_report_text, axis=1)

"""# 4. Data loading and preparation"""

import os
import pandas as pd
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


df = pd.read_csv(csv_file)
df = df.fillna(0)
df.replace(-1, 0, inplace=True)
print(df.head())

labels = df.columns[2:]
target_names = labels.tolist()
label_counts = df[labels].sum(axis=0)
total_counts = len(df)
label_counts = label_counts.replace(0, 1)
pos_weights = torch.tensor((total_counts - label_counts) / label_counts).float().to(device)

print(f"number of labels: {len(target_names)}")
print(target_names)

def build_report_path(row):
    subject_id_str = str(int(row["subject_id"])).zfill(8)
    study_id_str = str(int(row["study_id"]))
    path = os.path.join(
        reports_dir,
        f"p{subject_id_str[:2]}",
        f"p{subject_id_str}",
        f"s{study_id_str}.txt"
    )
    return path

df["report_path"] = df.apply(build_report_path, axis=1)
def read_report_text(row):
    try:
        with open(row["report_path"], "r") as f:
            return f.read()
    except FileNotFoundError:
        return
df["report_text"] = df.apply(read_report_text, axis=1)
print(df[["subject_id", "study_id", "report_path", "report_text"]].head())
df.head()

"""# 5. Modelling

## 5.1 Define dataset class
"""

from torch.utils.data import Dataset
from transformers import BertTokenizer

class CheXpertDataset(Dataset):
    def __init__(self, df, tokenizer, max_length=512):
        self.texts = df['report_text'].tolist()
        self.labels = df[labels].values
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]

        encoding = self.tokenizer(
            text,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        item = {key: val.squeeze(0) for key, val in encoding.items()}
        item["labels"] = torch.tensor(self.labels[idx], dtype=torch.float)
        return item

"""## 5.2 Define the model and loss function

model.pyÔºöBERT
"""

import torch
print(torch.__version__)
print(torch.version.cuda)

import torch
import torch.nn as nn
from transformers import BertModel

class CheXpertClassifier(nn.Module):
    def __init__(self, num_labels):
        super(CheXpertClassifier, self).__init__()
        self.bert = BertModel.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")
        self.fc = nn.Linear(self.bert.config.hidden_size, num_labels)  # ËæìÂá∫Â±Ç

    def forward(self, input_ids, attention_mask):

        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)

        cls_output = outputs.pooler_output

        logits = self.fc(cls_output)
        return logits

class FocalLoss(torch.nn.Module):
    def __init__(self, alpha=1, gamma=2, reduction='mean', pos_weight=None):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        self.pos_weight = pos_weight

    def forward(self, inputs, targets):
        bce_loss = torch.nn.functional.binary_cross_entropy_with_logits(
            inputs, targets, reduction='none', pos_weight=self.pos_weight
        )
        probs = torch.sigmoid(inputs)
        pt = torch.where(targets == 1, probs, 1 - probs)
        focal_term = (1 - pt) ** self.gamma

        loss = self.alpha * focal_term * bce_loss

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss

"""#6. Model Training and Evaluation"""

import torch
from torch.optim import AdamW
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler, autocast
from sklearn.metrics import classification_report, f1_score
from tqdm import tqdm
import numpy as np

tokenizer = BertTokenizer.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")
model = CheXpertClassifier(num_labels=len(labels))
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# split
from sklearn.model_selection import train_test_split
train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)

# initialize
train_dataset = CheXpertDataset(train_df, tokenizer)
val_dataset = CheXpertDataset(val_df, tokenizer)

batch_size = 64

#optimize
num_workers = 2

train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=num_workers,
    pin_memory=True,
    prefetch_factor=3,
    persistent_workers=True if num_workers > 0 else False
)

val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size*2,
    num_workers=2,
    pin_memory=True
)


optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-5)

from transformers import get_linear_schedule_with_warmup

epochs = 3

total_steps = len(train_loader) * epochs


num_warmup_steps = int(0.1 * total_steps)
num_training_steps = total_steps * 2

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=num_warmup_steps,
    num_training_steps=num_training_steps
)


loss_fn = FocalLoss(alpha=1, gamma=2, pos_weight=pos_weights)

# loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weights)
scaler = GradScaler()

def train_model(model, train_loader, val_loader, optimizer, loss_fn, epochs=3):
    best_val_f1 = 0
    history = {'train_loss': [], 'val_f1': []}

    for epoch in range(epochs):
        model.train()
        epoch_loss = 0
        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Training]', leave=False)

        for batch in progress_bar:
            input_ids = batch["input_ids"].to(device, non_blocking=True)
            attention_mask = batch["attention_mask"].to(device, non_blocking=True)
            labels = batch["labels"].to(device, non_blocking=True)

            optimizer.zero_grad(set_to_none=True)

            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):
                logits = model(input_ids, attention_mask)
                loss = loss_fn(logits, labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()

            epoch_loss += loss.item()
            progress_bar.set_postfix({'batch_loss': f'{loss.item():.4f}'})

        avg_train_loss = epoch_loss / len(train_loader)
        history['train_loss'].append(avg_train_loss)


        val_f1 = evaluate_model(model, val_loader, device)
        history['val_f1'].append(val_f1)


        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            torch.save(model.state_dict(), 'best_model.pth')

        print(f'\nEpoch {epoch+1} Summary:')
        print(f'Train Loss: {avg_train_loss:.4f} | Val F1: {val_f1:.4f}')
        print('-'*50)

        print(f"Current LR: {scheduler.get_last_lr()[0]:.2e}")


    return history

from sklearn.metrics import f1_score
import torch

@torch.no_grad()
def evaluate_model(model, data_loader, device):
    model.eval()
    all_preds, all_labels = [], []

    for batch in tqdm(data_loader, desc='Evaluating', leave=False):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):
            logits = model(input_ids, attention_mask)

        probs = torch.sigmoid(logits)
        threshold = 0.3
        preds = (probs > threshold).int()

        all_preds.append(preds.cpu())
        all_labels.append(labels.cpu())

    all_preds = torch.cat(all_preds)
    all_labels = torch.cat(all_labels)


    f1 = f1_score(all_labels.numpy(), all_preds.numpy(), average='macro')
    return f1

import torch
import gc
torch.cuda.empty_cache()
gc.collect()

history = train_model(model, train_loader, val_loader, optimizer, loss_fn, epochs=3)

print("\nFinal Evaluation:")
model.load_state_dict(torch.load('best_model.pth'))
model.eval()
all_preds, all_labels = [], []

with torch.no_grad():
    for batch in tqdm(val_loader, desc='Final Evaluation'):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        logits = model(input_ids, attention_mask)
        preds = torch.sigmoid(logits) > 0.5

        all_preds.append(preds.cpu())
        all_labels.append(labels.cpu())

all_preds = torch.cat(all_preds)
all_labels = torch.cat(all_labels)

from sklearn.metrics import classification_report


assert all_preds.shape[1] == len(target_names), "Ê†áÁ≠æÊï∞Èáè‰∏éÈ¢ÑÊµãÁª¥Â∫¶‰∏çÂåπÈÖçÔºÅ"
print("\nClassification Report:")
print(classification_report(all_labels.numpy(), all_preds.numpy(), target_names=target_names))



import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(history['train_loss'], label='Training Loss')
plt.plot(history['val_f1'], label='Validation F1')
plt.title('Training Progress')
plt.xlabel('Epochs')
plt.legend()
plt.show()

df['report_len'] = df['report_text'].apply(lambda x: len(x.strip()))
print(df['report_len'].describe())
print(df['report_text'].sample(5).tolist())

"""It indicates that the length distribution of report_text is very reasonable.
Indicator meaning
min = 148. No blank text (excellent)
mean ‚âà 635. The report content is moderate and not too short
max ‚âà 2773. There are some extremely long texts, but they are within the normal range
std ‚âà 254 indicates that the length fluctuation is moderate and there is no obvious structural anomaly in the reporting style

No changes are needed.
"""

print(f"label nums: {len(target_names)}")
print(target_names)

"""#7. Visualization

## F1-score for each categories
"""

from sklearn.metrics import classification_report
import matplotlib.pyplot as plt

report = classification_report(all_labels, all_preds, target_names=target_names, output_dict=True)
class_f1 = {k: v['f1-score'] for k, v in report.items() if k in target_names}

plt.figure(figsize=(10,8))
plt.barh(list(class_f1.keys()), list(class_f1.values()))
plt.title('F1 Score per Class')
plt.xlabel('F1 Score')
plt.xlim(0, 1)
plt.grid(axis='x', linestyle='--')
plt.show()

"""## Category distribution and prediction comparison"""

import numpy as np

true_pos_ratio = all_labels.numpy().mean(axis=0)
pred_pos_ratio = all_preds.numpy().mean(axis=0)

plt.figure(figsize=(12,6))
x = np.arange(len(target_names))
width = 0.35

plt.bar(x - width/2, true_pos_ratio, width, label='True')
plt.bar(x + width/2, pred_pos_ratio, width, label='Predicted')

plt.xticks(x, target_names, rotation=45, ha='right')
plt.ylabel('Positive Ratio')
plt.title('True vs Predicted Positive Ratios')
plt.legend()
plt.tight_layout()
plt.show()