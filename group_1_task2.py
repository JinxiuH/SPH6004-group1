# -*- coding: utf-8 -*-
"""Group 1_Task2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fmCjdODtqyGDDV4ud7gwCsaPXoY35Jxo

# 1. Environment setup and data preparation
"""

#!pip install transformers datasets scikit-learn pandas tqdm

from google.colab import drive
drive.mount('/content/drive/')

!unzip "/content/drive/MyDrive/SPH6004-group assignment/mimic-cxr-reports.zip"

reports_dir = "/content/files"
csv_file = "/content/drive/MyDrive/SPH6004-group assignment/mimic-cxr-2.0.0-chexpert.csv"

"""# 2. Test data loading and directory structure viewing"""

import os

for subfolder1 in os.listdir(reports_dir):
    subfolder1_path = os.path.join(reports_dir, subfolder1)
    if os.path.isdir(subfolder1_path):
        print(f"🔹 {subfolder1}/")
        for subfolder2 in os.listdir(subfolder1_path):
            subfolder2_path = os.path.join(subfolder1_path, subfolder2)
            print(f"    └── {subfolder2}/")
            for file in os.listdir(subfolder2_path):
                if file.endswith(".txt"):
                    print(f"        📄 {file}")
                    break
            break

"""# 3. Data preprocessing and label conversion"""

print(f"Reports directory: {reports_dir}")
import os
import pandas as pd
for subfolder1 in os.listdir(reports_dir):
    print(subfolder1)

import os
import pandas as pd

df = pd.read_csv(csv_file)
df = df.fillna(0)
df.replace(-1, 0, inplace=True)
labels = df.columns[2:]

def read_report_text(row):
    subject_id = str(row['subject_id']).zfill(8)
    study_id = str(row['study_id'])
    report_path = os.path.join(
        reports_dir,
        f"p{subject_id[:2]}",
        f"p{subject_id}",
        f"s{study_id}.txt"
    )
    try:
        with open(report_path, "r") as file:
            return file.read()
    except FileNotFoundError:
        return

df["report_path"] = df.apply(lambda row: os.path.join(
    reports_dir,
    f"p{str(row['subject_id']).zfill(8)[:2]}",
    f"p{str(row['subject_id']).zfill(8)}",
    f"s{int(row['study_id'])}.txt"
), axis=1)

print(df["report_path"].head())

df['report_text'] = df.apply(read_report_text, axis=1)

"""# 4. Data loading and preparation"""

import os
import pandas as pd
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


df = pd.read_csv(csv_file)
df = df.fillna(0)
df.replace(-1, 0, inplace=True)
print(df.head())

labels = df.columns[2:]
target_names = labels.tolist()
label_counts = df[labels].sum(axis=0)
total_counts = len(df)
label_counts = label_counts.replace(0, 1)
pos_weights = torch.tensor((total_counts - label_counts) / label_counts).float().to(device)

print(f"number of labels: {len(target_names)}")
print(target_names)

def build_report_path(row):
    subject_id_str = str(int(row["subject_id"])).zfill(8)
    study_id_str = str(int(row["study_id"]))
    path = os.path.join(
        reports_dir,
        f"p{subject_id_str[:2]}",
        f"p{subject_id_str}",
        f"s{study_id_str}.txt"
    )
    return path

df["report_path"] = df.apply(build_report_path, axis=1)
def read_report_text(row):
    try:
        with open(row["report_path"], "r") as f:
            return f.read()
    except FileNotFoundError:
        return
df["report_text"] = df.apply(read_report_text, axis=1)
print(df[["subject_id", "study_id", "report_path", "report_text"]].head())
df.head()

"""# 5. Modelling

## 5.1 Define dataset class
"""

from torch.utils.data import Dataset
from transformers import BertTokenizer

class CheXpertDataset(Dataset):
    def __init__(self, df, tokenizer, max_length=512):
        self.texts = df['report_text'].tolist()
        self.labels = df[labels].values
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]

        encoding = self.tokenizer(
            text,
            padding="max_length",
            truncation=True,
            max_length=self.max_length,
            return_tensors="pt"
        )

        item = {key: val.squeeze(0) for key, val in encoding.items()}
        item["labels"] = torch.tensor(self.labels[idx], dtype=torch.float)
        return item

"""## 5.2 Define the model and loss function

model.py：BERT
"""

import torch
print(torch.__version__)
print(torch.version.cuda)

import torch
import torch.nn as nn
from transformers import BertModel

class CheXpertClassifier(nn.Module):
    def __init__(self, num_labels):
        super(CheXpertClassifier, self).__init__()
        self.bert = BertModel.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")
        self.fc = nn.Linear(self.bert.config.hidden_size, num_labels)  # 输出层

    def forward(self, input_ids, attention_mask):

        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)

        cls_output = outputs.pooler_output

        logits = self.fc(cls_output)
        return logits

class FocalLoss(torch.nn.Module):
    def __init__(self, alpha=1, gamma=2, reduction='mean', pos_weight=None):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        self.pos_weight = pos_weight

    def forward(self, inputs, targets):
        bce_loss = torch.nn.functional.binary_cross_entropy_with_logits(
            inputs, targets, reduction='none', pos_weight=self.pos_weight
        )
        probs = torch.sigmoid(inputs)
        pt = torch.where(targets == 1, probs, 1 - probs)
        focal_term = (1 - pt) ** self.gamma

        loss = self.alpha * focal_term * bce_loss

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:
            return loss

"""#6. Model Training and Evaluation"""

import torch
from torch.optim import AdamW
from torch.utils.data import DataLoader
from torch.cuda.amp import GradScaler, autocast
from sklearn.metrics import classification_report, f1_score
from tqdm import tqdm
import numpy as np

tokenizer = BertTokenizer.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")
model = CheXpertClassifier(num_labels=len(labels))
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# split
from sklearn.model_selection import train_test_split
train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)

# initialize
train_dataset = CheXpertDataset(train_df, tokenizer)
val_dataset = CheXpertDataset(val_df, tokenizer)

batch_size = 64

#optimize
num_workers = 2

train_loader = DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=num_workers,
    pin_memory=True,
    prefetch_factor=3,
    persistent_workers=True if num_workers > 0 else False
)

val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size*2,
    num_workers=2,
    pin_memory=True
)


optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=1e-5)

from transformers import get_linear_schedule_with_warmup

epochs = 3

total_steps = len(train_loader) * epochs


num_warmup_steps = int(0.1 * total_steps)
num_training_steps = total_steps * 2

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=num_warmup_steps,
    num_training_steps=num_training_steps
)


loss_fn = FocalLoss(alpha=1, gamma=2, pos_weight=pos_weights)

# loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weights)
scaler = GradScaler()

def train_model(model, train_loader, val_loader, optimizer, loss_fn, epochs=3):
    best_val_f1 = 0
    history = {'train_loss': [], 'val_f1': []}

    for epoch in range(epochs):
        model.train()
        epoch_loss = 0
        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Training]', leave=False)

        for batch in progress_bar:
            input_ids = batch["input_ids"].to(device, non_blocking=True)
            attention_mask = batch["attention_mask"].to(device, non_blocking=True)
            labels = batch["labels"].to(device, non_blocking=True)

            optimizer.zero_grad(set_to_none=True)

            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):
                logits = model(input_ids, attention_mask)
                loss = loss_fn(logits, labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            scheduler.step()

            epoch_loss += loss.item()
            progress_bar.set_postfix({'batch_loss': f'{loss.item():.4f}'})

        avg_train_loss = epoch_loss / len(train_loader)
        history['train_loss'].append(avg_train_loss)


        val_f1 = evaluate_model(model, val_loader, device)
        history['val_f1'].append(val_f1)


        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            torch.save(model.state_dict(), 'best_model.pth')

        print(f'\nEpoch {epoch+1} Summary:')
        print(f'Train Loss: {avg_train_loss:.4f} | Val F1: {val_f1:.4f}')
        print('-'*50)

        print(f"Current LR: {scheduler.get_last_lr()[0]:.2e}")


    return history

from sklearn.metrics import f1_score
import torch

@torch.no_grad()
def evaluate_model(model, data_loader, device):
    model.eval()
    all_preds, all_labels = [], []

    for batch in tqdm(data_loader, desc='Evaluating', leave=False):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):
            logits = model(input_ids, attention_mask)

        probs = torch.sigmoid(logits)
        threshold = 0.3
        preds = (probs > threshold).int()

        all_preds.append(preds.cpu())
        all_labels.append(labels.cpu())

    all_preds = torch.cat(all_preds)
    all_labels = torch.cat(all_labels)


    f1 = f1_score(all_labels.numpy(), all_preds.numpy(), average='macro')
    return f1

import torch
import gc
torch.cuda.empty_cache()
gc.collect()

history = train_model(model, train_loader, val_loader, optimizer, loss_fn, epochs=3)

print("\nFinal Evaluation:")
model.load_state_dict(torch.load('best_model.pth'))
model.eval()
all_preds, all_labels = [], []

with torch.no_grad():
    for batch in tqdm(val_loader, desc='Final Evaluation'):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)

        logits = model(input_ids, attention_mask)
        preds = torch.sigmoid(logits) > 0.5

        all_preds.append(preds.cpu())
        all_labels.append(labels.cpu())

all_preds = torch.cat(all_preds)
all_labels = torch.cat(all_labels)

from sklearn.metrics import classification_report


assert all_preds.shape[1] == len(target_names), "标签数量与预测维度不匹配！"
print("\nClassification Report:")
print(classification_report(all_labels.numpy(), all_preds.numpy(), target_names=target_names))



import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(history['train_loss'], label='Training Loss')
plt.plot(history['val_f1'], label='Validation F1')
plt.title('Training Progress')
plt.xlabel('Epochs')
plt.legend()
plt.show()

df['report_len'] = df['report_text'].apply(lambda x: len(x.strip()))
print(df['report_len'].describe())
print(df['report_text'].sample(5).tolist())

"""It indicates that the length distribution of report_text is very reasonable.
Indicator meaning
min = 148. No blank text (excellent)
mean ≈ 635. The report content is moderate and not too short
max ≈ 2773. There are some extremely long texts, but they are within the normal range
std ≈ 254 indicates that the length fluctuation is moderate and there is no obvious structural anomaly in the reporting style

No changes are needed.
"""

print(f"label nums: {len(target_names)}")
print(target_names)

"""#7. Visualization

## F1-score for each categories
"""

from sklearn.metrics import classification_report
import matplotlib.pyplot as plt

report = classification_report(all_labels, all_preds, target_names=target_names, output_dict=True)
class_f1 = {k: v['f1-score'] for k, v in report.items() if k in target_names}

plt.figure(figsize=(10,8))
plt.barh(list(class_f1.keys()), list(class_f1.values()))
plt.title('F1 Score per Class')
plt.xlabel('F1 Score')
plt.xlim(0, 1)
plt.grid(axis='x', linestyle='--')
plt.show()

"""## Category distribution and prediction comparison"""

import numpy as np

true_pos_ratio = all_labels.numpy().mean(axis=0)
pred_pos_ratio = all_preds.numpy().mean(axis=0)

plt.figure(figsize=(12,6))
x = np.arange(len(target_names))
width = 0.35

plt.bar(x - width/2, true_pos_ratio, width, label='True')
plt.bar(x + width/2, pred_pos_ratio, width, label='Predicted')

plt.xticks(x, target_names, rotation=45, ha='right')
plt.ylabel('Positive Ratio')
plt.title('True vs Predicted Positive Ratios')
plt.legend()
plt.tight_layout()
plt.show()